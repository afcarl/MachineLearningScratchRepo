{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Predictors of the World Series Winners?\n",
    "Of the teams that make the playoffs, is there a way to predict who the world series winner will be?\n",
    "\n",
    "Features - usually more informative to do ratios between values instead of just straight up values.\n",
    "* how many times in playoffs in last 5 years\n",
    "* Run differential? \n",
    "* salary\n",
    "* all star/cy young?\n",
    "* win percentage in second half of season\n",
    "* hot pitcher(s) - K/BB, FIP, WHIP, IP/GP, ERA of pitcher\n",
    "\n",
    "Different approaches:  \n",
    "* Maybe predict who will be in the world series final instead? Maybe it's too hard to know who will actually win. Rank teams in various categories relative to the other teams in your division.\n",
    "* Maybe some world series come down to chance, whereas others are more deterministic. For example, if the world series champions had game 7's in each series, then a slight perturbation to this system causes them not to ultimately win the WS. However maybe other world series are much more clear in who will win.  \n",
    "* Probably need a more complicated model, i.e. see the playoff bracket and predict the outcomes, having weights at each node. Probably a job for the Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data\n",
    "Collect, clean, organize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data and grab columns of interest\n",
    "team = pd.read_csv(\"csv/team.csv\")\n",
    "\n",
    "del_columns = [\"div_id\",\"name\",\"team_id_lahman45\",\"franchise_id\",\"team_id_retro\",\"team_id_br\",\"ppf\",\"bpf\",\"park\"]\n",
    "team.drop(del_columns, axis=1, inplace=True)\n",
    "\n",
    "year_cutoff = 1969\n",
    "team = team[team[\"year\"]>=year_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "team.loc[team[\"ws_win\"] == \"Y\", \"ws_win\"] = 1\n",
    "team.loc[team[\"ws_win\"] == \"N\", \"ws_win\"] = 0\n",
    "team.loc[pd.isnull(team[\"ws_win\"]), \"ws_win\"] = 0\n",
    "\n",
    "team.loc[team[\"wc_win\"] == \"Y\", \"wc_win\"] = 1\n",
    "team.loc[team[\"wc_win\"] == \"N\", \"wc_win\"] = 0\n",
    "team.loc[pd.isnull(team[\"wc_win\"]), \"wc_win\"] = 0\n",
    "\n",
    "team.loc[team[\"lg_win\"] == \"Y\", \"lg_win\"] = 1\n",
    "team.loc[team[\"lg_win\"] == \"N\", \"lg_win\"] = 0\n",
    "team.loc[pd.isnull(team[\"lg_win\"]), \"lg_win\"] = 0\n",
    "\n",
    "team.loc[team[\"div_win\"] == \"Y\", \"div_win\"] = 1\n",
    "team.loc[team[\"div_win\"] == \"N\", \"div_win\"] = 0\n",
    "team.loc[pd.isnull(team[\"div_win\"]), \"div_win\"] = 0\n",
    "\n",
    "team[\"postseason\"] = 0\n",
    "team.loc[(team[\"div_win\"]==1) | (team[\"wc_win\"]==1),\"postseason\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>league_id</th>\n",
       "      <th>team_id</th>\n",
       "      <th>rank</th>\n",
       "      <th>g</th>\n",
       "      <th>ghome</th>\n",
       "      <th>w</th>\n",
       "      <th>l</th>\n",
       "      <th>div_win</th>\n",
       "      <th>wc_win</th>\n",
       "      <th>...</th>\n",
       "      <th>ipouts</th>\n",
       "      <th>ha</th>\n",
       "      <th>hra</th>\n",
       "      <th>bba</th>\n",
       "      <th>soa</th>\n",
       "      <th>e</th>\n",
       "      <th>dp</th>\n",
       "      <th>fp</th>\n",
       "      <th>attendance</th>\n",
       "      <th>postseason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>1969</td>\n",
       "      <td>NL</td>\n",
       "      <td>ATL</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>81.0</td>\n",
       "      <td>93</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4335</td>\n",
       "      <td>1334</td>\n",
       "      <td>144</td>\n",
       "      <td>438</td>\n",
       "      <td>893</td>\n",
       "      <td>115</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1458320.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>1969</td>\n",
       "      <td>AL</td>\n",
       "      <td>BAL</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>81.0</td>\n",
       "      <td>109</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4419</td>\n",
       "      <td>1194</td>\n",
       "      <td>117</td>\n",
       "      <td>498</td>\n",
       "      <td>897</td>\n",
       "      <td>101</td>\n",
       "      <td>145.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1062069.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>1969</td>\n",
       "      <td>AL</td>\n",
       "      <td>BOS</td>\n",
       "      <td>3</td>\n",
       "      <td>162</td>\n",
       "      <td>81.0</td>\n",
       "      <td>87</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4398</td>\n",
       "      <td>1423</td>\n",
       "      <td>155</td>\n",
       "      <td>685</td>\n",
       "      <td>935</td>\n",
       "      <td>157</td>\n",
       "      <td>178.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1833246.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>1969</td>\n",
       "      <td>AL</td>\n",
       "      <td>CAL</td>\n",
       "      <td>3</td>\n",
       "      <td>163</td>\n",
       "      <td>81.0</td>\n",
       "      <td>71</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4314</td>\n",
       "      <td>1294</td>\n",
       "      <td>126</td>\n",
       "      <td>517</td>\n",
       "      <td>885</td>\n",
       "      <td>135</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.97</td>\n",
       "      <td>758388.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>1969</td>\n",
       "      <td>AL</td>\n",
       "      <td>CHA</td>\n",
       "      <td>5</td>\n",
       "      <td>162</td>\n",
       "      <td>81.0</td>\n",
       "      <td>68</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4311</td>\n",
       "      <td>1470</td>\n",
       "      <td>146</td>\n",
       "      <td>564</td>\n",
       "      <td>810</td>\n",
       "      <td>122</td>\n",
       "      <td>163.0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>589546.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year league_id team_id  rank    g  ghome    w   l div_win wc_win  \\\n",
       "1517  1969        NL     ATL     1  162   81.0   93  69       1      0   \n",
       "1518  1969        AL     BAL     1  162   81.0  109  53       1      0   \n",
       "1519  1969        AL     BOS     3  162   81.0   87  75       0      0   \n",
       "1520  1969        AL     CAL     3  163   81.0   71  91       0      0   \n",
       "1521  1969        AL     CHA     5  162   81.0   68  94       0      0   \n",
       "\n",
       "         ...     ipouts    ha  hra  bba  soa    e     dp    fp  attendance  \\\n",
       "1517     ...       4335  1334  144  438  893  115  114.0  0.98   1458320.0   \n",
       "1518     ...       4419  1194  117  498  897  101  145.0  0.98   1062069.0   \n",
       "1519     ...       4398  1423  155  685  935  157  178.0  0.97   1833246.0   \n",
       "1520     ...       4314  1294  126  517  885  135  164.0  0.97    758388.0   \n",
       "1521     ...       4311  1470  146  564  810  122  163.0  0.98    589546.0   \n",
       "\n",
       "      postseason  \n",
       "1517           1  \n",
       "1518           1  \n",
       "1519           0  \n",
       "1520           0  \n",
       "1521           0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Looks like they only made divisions in 1969. \n",
    "team.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "Especially for this type of problem, feature selection is by far the most important part. Using the standard columns in this dataframe yields a terrible fit using a random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "team_names = team[\"team_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Number of playoff appearances in the last 5 years\n",
    "team[\"5yrpost\"] = 0\n",
    "for t in team_names:\n",
    "    team_t = team[team[\"team_id\"]==t]\n",
    "    team.loc[team[\"team_id\"]==t,\"5yrpost\"] = team_t[\"postseason\"].rolling(window=5,min_periods=2).sum()\n",
    "team.loc[team[\"5yrpost\"].isnull(),\"5yrpost\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ratios - X for / X against\n",
    "team[\"r_r\"] = team[\"r\"]/team[\"er\"]  #runs\n",
    "team[\"h_r\"] = team[\"h\"]/team[\"ha\"]  #hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitching Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pitching = pd.read_csv(\"csv/pitching.csv\")\n",
    "pitching = pitching[pitching[\"year\"]>=year_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#whip - anything below 1.1 is considered great, below 1 is outstanding. Check for # ipouts though!!\n",
    "#SO/BB - ratio of strikeouts to walks. \n",
    "#prevent inf values\n",
    "pitching.loc[pitching[\"ipouts\"]==0, \"ipouts\"] = 1\n",
    "pitching.loc[pitching[\"bb\"]==0, \"bb\"] = 1\n",
    "\n",
    "pitching[\"whip\"] = (pitching[\"bb\"] - pitching[\"ibb\"] + pitching[\"h\"]) / (pitching[\"ipouts\"]/3.)\n",
    "pitching[\"so/bb\"] = pitching[\"so\"]/pitching[\"bb\"]\n",
    "pitching[\"ip/g\"] = pitching[\"ipouts\"]/(3*pitching[\"g\"])\n",
    "pitching.loc[pd.isnull(pitching[\"ip/g\"]),\"ip/g\"] = 0\n",
    "\n",
    "pitching[\"whip\"] = (pitching[\"whip\"] - pitching[\"whip\"].mean())/pitching[\"whip\"].std()\n",
    "pitching[\"so/bb\"] = (pitching[\"so/bb\"] - pitching[\"so/bb\"].mean())/pitching[\"so/bb\"].std()\n",
    "#pitching[\"ip/g\"] = (pitching[\"ip/g\"] - pitching[\"ip/g\"].mean())/pitching[\"ip/g\"].std()\n",
    "\n",
    "pitching[\"hot_pitcher\"] = 0\n",
    "sdfm_thresh = 1              #number of standard deviations from the mean to define a \"hot pitcher\"\n",
    "pitching.loc[(pitching[\"whip\"]<= -sdfm_thresh)|(pitching[\"so/bb\"]>=sdfm_thresh)|(pitching[\"ip/g\"]>=7),\"hot_pitcher\"]=1\n",
    "pitching.loc[(pitching[\"w\"]<10) & (pitching[\"sv\"]<15),\"hot_pitcher\"] = 0       #IP*3 > 100 and g > 10\n",
    "#pitching[pitching[\"ip/g\"]>7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert pitching stats into team stats\n",
    "pitching=pitching.groupby([\"team_id\",\"year\"]).sum()\n",
    "pitching = pitching.reset_index()\n",
    "team[\"n_hp\"] = 0\n",
    "\n",
    "for t in team_names:\n",
    "    team.loc[team[\"team_id\"]==t,\"n_hp\"] = pitching.loc[pitching[\"team_id\"]==t,\"hot_pitcher\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>team_id</th>\n",
       "      <th>w</th>\n",
       "      <th>era</th>\n",
       "      <th>sv</th>\n",
       "      <th>postseason</th>\n",
       "      <th>5yrpost</th>\n",
       "      <th>n_hp</th>\n",
       "      <th>h_r</th>\n",
       "      <th>r_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>1969</td>\n",
       "      <td>ATL</td>\n",
       "      <td>93</td>\n",
       "      <td>3.53</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057721</td>\n",
       "      <td>1.218695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>1969</td>\n",
       "      <td>BAL</td>\n",
       "      <td>109</td>\n",
       "      <td>2.83</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.226968</td>\n",
       "      <td>1.682505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>1969</td>\n",
       "      <td>BOS</td>\n",
       "      <td>87</td>\n",
       "      <td>3.92</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.970485</td>\n",
       "      <td>1.162754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>1969</td>\n",
       "      <td>CAL</td>\n",
       "      <td>71</td>\n",
       "      <td>3.54</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.943586</td>\n",
       "      <td>0.932862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>1969</td>\n",
       "      <td>CHA</td>\n",
       "      <td>68</td>\n",
       "      <td>4.21</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.915646</td>\n",
       "      <td>0.930060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year team_id    w   era  sv  postseason  5yrpost  n_hp       h_r  \\\n",
       "1517  1969     ATL   93  3.53  42           1      0.0     1  1.057721   \n",
       "1518  1969     BAL  109  2.83  36           1      0.0     1  1.226968   \n",
       "1519  1969     BOS   87  3.92  41           0      0.0     1  0.970485   \n",
       "1520  1969     CAL   71  3.54  39           0      0.0     0  0.943586   \n",
       "1521  1969     CHA   68  4.21  25           0      0.0     0  0.915646   \n",
       "\n",
       "           r_r  \n",
       "1517  1.218695  \n",
       "1518  1.682505  \n",
       "1519  1.162754  \n",
       "1520  0.932862  \n",
       "1521  0.930060  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "team.loc[:,[\"year\",\"team_id\",\"w\",\"era\",\"sv\",\"postseason\",\"5yrpost\",\"n_hp\",\"h_r\",\"r_r\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most interesting question to ask is \"Out of the teams that made the playoffs, who is most likely to win the world series?\". So, we need to filter out all the teams that didn't make the playoffs each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "team = team.loc[team[\"postseason\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab only the features we consider most predictive of world series performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xcolumns = [\"w\",\"r\",\"sv\",\"era\",\"fp\",\"5yrpost\",\"n_hp\",\"h_r\",\"r_r\"]\n",
    "#Xcolumns = [\"w\",\"n_hp\",\"h_r\",\"r_r\",\"5yrpost\"]\n",
    "y = team[\"ws_win\"].astype(int)\n",
    "X = team[Xcolumns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some ML methods require that all the features are scaled, so we will use the Xs dataframe for scaled values of all our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X)\n",
    "Xs = scaler.transform(X)    #scale data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "Dimensionality reduction is always key to an efficient algorithm. It is also very useful to determine which features have the greatest importance.\n",
    "\n",
    "Each principal component is a linear combination of the original features:  \n",
    "$PC^i = B^i_1X_1 + B^i_2X_2 + ... + B^i_nX_n$  \n",
    "\n",
    "where $B_j$ are the weights and $X_i$ are the features. Thus the (absolute value of the) weights correspond to feature importance for constructing the component of maximal variance. As can be seen below, it looks like there is no single dominating feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.482786</td>\n",
       "      <td>-0.343840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.162682</td>\n",
       "      <td>-0.601172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>0.074016</td>\n",
       "      <td>-0.366596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>era</th>\n",
       "      <td>-0.342424</td>\n",
       "      <td>-0.508295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fp</th>\n",
       "      <td>-0.160620</td>\n",
       "      <td>-0.255035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5yrpost</th>\n",
       "      <td>0.095633</td>\n",
       "      <td>-0.123802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_hp</th>\n",
       "      <td>0.122726</td>\n",
       "      <td>0.194373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_r</th>\n",
       "      <td>0.467098</td>\n",
       "      <td>0.015233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r_r</th>\n",
       "      <td>0.591213</td>\n",
       "      <td>0.096053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1\n",
       "w        0.482786 -0.343840\n",
       "r        0.162682 -0.601172\n",
       "sv       0.074016 -0.366596\n",
       "era     -0.342424 -0.508295\n",
       "fp      -0.160620 -0.255035\n",
       "5yrpost  0.095633 -0.123802\n",
       "n_hp     0.122726  0.194373\n",
       "h_r      0.467098  0.015233\n",
       "r_r      0.591213  0.096053"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(Xs)                 #get PCA of scaled data\n",
    "pd.DataFrame(pca.components_.T, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always good to actually look at the data and see if you can personally recognize any patterns. There's a good chance that if you can't recognize anything, the machine learning won't either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"e9920c0c-98b3-4189-8506-2bae61842856\" style=\"height: 525; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"e9920c0c-98b3-4189-8506-2bae61842856\", [{\"name\": \"ws winners\", \"marker\": {\"opacity\": 0.8, \"size\": 8}, \"mode\": \"markers\", \"y\": [0.0, 2.0, 2.0, 2.0, 3.0, 4.0, 3.0, 4.0, 2.0, 3.0, 2.0, 4.0, 3.0, 1.0, 2.0, 1.0, 3.0, 1.0, 1.0, 2.0, 2.0, 1.0, 2.0, 3.0, 4.0, 4.0, 2.0, 1.0, 4.0, 5.0, 5.0, 2.0, 1.0, 1.0, 2.0, 1.0, 4.0, 4.0, 2.0, 4.0, 1.0, 2.0, 2.0, 2.0, 3.0, 2.0], \"x\": [100, 108, 97, 93, 94, 90, 108, 102, 100, 100, 98, 91, 63, 92, 98, 104, 91, 108, 85, 94, 99, 91, 95, 96, 95, 90, 92, 92, 114, 98, 87, 92, 99, 91, 98, 99, 83, 96, 92, 103, 92, 90, 94, 97, 88, 95], \"z\": [2.99, 3.15, 3.31, 2.58, 3.29, 2.95, 3.37, 3.51, 3.61, 3.18, 3.41, 3.43, 3.01, 3.37, 3.63, 3.49, 3.49, 3.11, 4.63, 2.96, 3.09, 3.39, 3.69, 3.91, 4.21, 3.44, 4.65, 3.83, 3.82, 4.13, 4.76, 3.87, 3.69, 4.04, 4.18, 3.61, 4.54, 3.87, 3.88, 4.26, 3.36, 3.74, 3.68, 3.79, 3.5, 3.73], \"type\": \"scatter3d\"}, {\"name\": \"ws losers\", \"marker\": {\"opacity\": 0.8, \"size\": 4}, \"mode\": \"markers\", \"y\": [0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 3.0, 1.0, 1.0, 2.0, 1.0, 3.0, 4.0, 3.0, 2.0, 4.0, 1.0, 4.0, 1.0, 5.0, 4.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 1.0, 1.0, 3.0, 1.0, 4.0, 4.0, 2.0, 4.0, 1.0, 1.0, 4.0, 1.0, 4.0, 1.0, 2.0, 2.0, 1.0, 2.0, 3.0, 1.0, 3.0, 1.0, 3.0, 2.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 3.0, 3.0, 1.0, 1.0, 2.0, 2.0, 2.0, 4.0, 3.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 4.0, 2.0, 3.0, 1.0, 3.0, 2.0, 1.0, 4.0, 2.0, 1.0, 4.0, 2.0, 2.0, 2.0, 1.0, 5.0, 3.0, 5.0, 3.0, 1.0, 3.0, 5.0, 1.0, 2.0, 1.0, 2.0, 2.0, 2.0, 5.0, 4.0, 4.0, 5.0, 2.0, 3.0, 2.0, 3.0, 5.0, 1.0, 5.0, 3.0, 2.0, 3.0, 5.0, 2.0, 1.0, 2.0, 5.0, 4.0, 3.0, 2.0, 5.0, 2.0, 1.0, 3.0, 5.0, 4.0, 5.0, 3.0, 3.0, 0.0, 5.0, 1.0, 4.0, 1.0, 2.0, 4.0, 5.0, 1.0, 3.0, 2.0, 1.0, 2.0, 1.0, 1.0, 2.0, 5.0, 1.0, 4.0, 2.0, 2.0, 3.0, 3.0, 1.0, 1.0, 4.0, 2.0, 4.0, 3.0, 2.0, 3.0, 3.0, 1.0, 1.0, 3.0, 4.0, 4.0, 2.0, 1.0, 2.0, 1.0, 2.0, 4.0, 5.0, 3.0, 2.0, 2.0, 1.0, 2.0, 2.0, 4.0, 1.0, 3.0, 3.0, 1.0, 3.0, 3.0, 1.0, 3.0, 2.0, 2.0, 1.0, 4.0, 3.0, 2.0, 4.0, 1.0, 1.0, 2.0, 3.0, 2.0, 4.0, 2.0, 1.0, 3.0, 3.0, 1.0, 5.0, 3.0, 1.0, 1.0, 3.0], \"x\": [93, 109, 97, 102, 98, 89, 101, 101, 90, 95, 86, 96, 97, 99, 82, 91, 102, 88, 95, 98, 92, 90, 97, 101, 102, 98, 101, 92, 95, 90, 102, 88, 90, 93, 97, 103, 61, 50, 62, 60, 59, 64, 59, 89, 93, 95, 99, 91, 90, 96, 84, 92, 95, 101, 99, 95, 92, 96, 98, 90, 95, 89, 100, 104, 93, 92, 89, 88, 103, 95, 94, 98, 91, 98, 96, 96, 104, 94, 97, 86, 85, 100, 77, 78, 79, 79, 96, 88, 99, 90, 91, 88, 90, 101, 98, 86, 84, 96, 90, 90, 106, 92, 90, 89, 102, 98, 88, 100, 103, 94, 97, 97, 97, 95, 95, 95, 94, 91, 91, 97, 95, 88, 91, 93, 95, 102, 116, 93, 98, 101, 94, 103, 103, 95, 97, 101, 95, 88, 90, 101, 96, 100, 92, 96, 92, 93, 92, 101, 105, 90, 95, 89, 95, 95, 82, 100, 95, 88, 96, 97, 97, 93, 88, 90, 85, 96, 90, 94, 94, 89, 95, 89, 97, 100, 84, 90, 97, 95, 92, 97, 95, 87, 93, 91, 91, 91, 94, 95, 97, 96, 90, 94, 95, 96, 97, 102, 91, 96, 94, 93, 97, 88, 95, 94, 88, 93, 98, 96, 90, 92, 93, 92, 96, 94, 97, 92, 96, 90, 89, 98, 94, 88, 88, 90, 96, 93, 87, 88, 86, 100, 98, 97, 90, 92], \"z\": [3.53, 2.83, 3.24, 3.69, 3.23, 3.7, 2.99, 3.05, 3.32, 3.21, 2.96, 2.81, 3.07, 3.4, 3.26, 3.27, 2.97, 3.49, 3.98, 3.27, 3.01, 3.21, 3.19, 3.08, 3.52, 3.22, 3.71, 3.44, 3.12, 3.33, 3.26, 4.34, 3.58, 3.1, 3.83, 3.58, 2.66, 3.56, 3.91, 3.3, 2.9, 3.3, 4.05, 3.82, 3.82, 3.98, 3.67, 3.1, 3.34, 3.75, 3.92, 3.48, 2.96, 3.1, 3.31, 3.93, 3.84, 3.15, 4.02, 3.68, 3.91, 3.97, 2.91, 3.44, 3.43, 3.3, 3.58, 3.72, 3.18, 3.4, 3.49, 3.44, 3.5, 3.14, 3.73, 3.35, 3.14, 3.7, 3.95, 4.39, 4.03, 3.83, 4.97, 3.66, 4.56, 4.5, 3.54, 5.15, 4.35, 3.48, 3.73, 3.98, 4.66, 3.18, 3.91, 4.73, 3.67, 3.84, 4.79, 4.41, 3.25, 4.19, 4.5, 4.45, 3.5, 3.63, 5.0, 3.77, 3.63, 4.0, 4.9, 3.84, 4.27, 5.07, 4.05, 4.66, 4.16, 4.58, 4.49, 4.21, 4.38, 3.59, 4.64, 4.37, 4.02, 3.59, 3.54, 3.93, 3.92, 3.13, 4.12, 3.87, 3.68, 3.54, 3.7, 4.1, 4.48, 3.83, 4.41, 4.02, 3.63, 3.73, 4.28, 3.74, 4.05, 4.01, 4.03, 4.69, 3.75, 3.98, 4.74, 3.51, 3.68, 4.52, 4.13, 3.49, 3.84, 4.23, 3.95, 4.41, 4.14, 4.21, 3.87, 4.13, 4.04, 4.05, 4.32, 4.23, 4.49, 4.73, 4.01, 4.06, 3.87, 3.99, 3.68, 3.85, 3.82, 4.35, 4.22, 4.45, 3.41, 4.5, 4.16, 3.66, 3.56, 4.01, 3.95, 4.06, 3.67, 3.78, 3.93, 3.8, 4.04, 3.63, 3.73, 3.02, 3.58, 3.79, 3.42, 3.9, 3.34, 3.75, 3.84, 3.48, 3.71, 3.99, 3.33, 3.18, 3.38, 3.82, 3.61, 3.25, 3.56, 3.26, 3.42, 3.74, 3.43, 4.01, 3.51, 3.58, 3.4, 3.22, 3.47, 3.5, 3.03, 3.8, 4.03, 4.24, 3.57, 2.94, 3.21, 3.36, 3.43, 3.44], \"type\": \"scatter3d\"}], {\"margin\": {\"r\": 0, \"b\": 0, \"t\": 0, \"l\": 0}}, {\"linkText\": \"Export to plot.ly\", \"showLink\": true})});</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "arr = X[y==1]\n",
    "arr2 = X[y==0]\n",
    "c = [\"w\",\"5yrpost\",\"era\"]\n",
    "\n",
    "trace0 = go.Scatter3d(\n",
    "    x=arr[c[0]],\n",
    "    y=arr[c[1]],\n",
    "    z=arr[c[2]],\n",
    "    mode='markers',\n",
    "    name='ws winners',\n",
    "    marker=dict(\n",
    "        size=8,\n",
    "        #color=y,             # set color to an array/list of desired values\n",
    "        #colorscale='RdGy',   # choose a colorscale\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "trace1 = go.Scatter3d(\n",
    "    x=arr2[c[0]],\n",
    "    y=arr2[c[1]],\n",
    "    z=arr2[c[2]],\n",
    "    mode='markers',\n",
    "    name='ws losers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        #color=y,             # set color to an array/list of desired values\n",
    "        #colorscale='RdGy',   # choose a colorscale\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "data = [trace0,trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split,cross_val_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "Xs_train, Xs_test, ys_train, ys_test = train_test_split(Xs, y, test_size=0.25, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### An important note about Train/Test/CV\n",
    "Usually you split your data into 3 chunks: Train, test, and validation sets:  \n",
    "1) Training set: The gold standard where you know the answers. Train your model with this set.  \n",
    "2) Validation set: Here you also know the answers, and want to test the accuracy of your model using some kind of scoring system. This lets you know if you've overfit/underfit your model to the training set. From scikit-learn:   \n",
    "*When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.*  \n",
    "To avoid splitting your data endlessly leaving each set with very few data points, CV is introduced, merging the training and validation set into one and doing internal splitting. But you still need to reserve a final test set. The CV is done on the training set.  \n",
    "3) Test set: This is the final test of your model to see how well it performs.  \n",
    "\n",
    "Apparently, the K-fold CV score in general is the best method for determining your predictive accuracy.\n",
    "\n",
    "ref: http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from gridsearch: {{'kernel': 'poly', 'C': 20, 'degree': 2}} with a score of 0.5809\n"
     ]
    }
   ],
   "source": [
    "#We learned from RF that accuracy score is not a good metric for predicting world series winners. AUC is better.\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "cv_s = StratifiedShuffleSplit(ys_train,  n_iter=10 , test_size=0.1, random_state=42)\n",
    "clf=SVC()\n",
    "param_grid = {'kernel':('linear', 'rbf','poly'), 'C':[1,10,20], 'degree':[2,3,5]}\n",
    "CV_svm = GridSearchCV(n_jobs=-1, estimator=clf, scoring=\"roc_auc\", param_grid=param_grid, cv=cv_s)\n",
    "CV_svm.fit(Xs_train, ys_train)\n",
    "print(\"Best Parameters from gridsearch: {%s} with a score of %0.4f\" % (CV_svm.best_params_, CV_svm.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87142857142857144"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#might be good to find weights for the positive world series winners.\n",
    "from sklearn import metrics\n",
    "model_svm = CV_svm.best_estimator_\n",
    "model_svm.score(Xs_test, ys_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters from gridsearch: {{'max_features': 'sqrt', 'n_estimators': 600, 'min_samples_leaf': 10}} with a score of 0.6490\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "cv_s = StratifiedShuffleSplit(y_train,  n_iter=10 , test_size=0.1, random_state=42)\n",
    "rfc = RandomForestClassifier(max_features= 'auto' ,n_estimators=50) \n",
    "param_grid = { \n",
    "        'n_estimators': [600],\n",
    "        'max_features': ['sqrt'],\n",
    "        'min_samples_leaf': [10]}\n",
    "CV_rfc = GridSearchCV(n_jobs=-1, estimator=rfc, scoring=\"roc_auc\", param_grid=param_grid, cv=cv_s)\n",
    "CV_rfc.fit(X_train, y_train);\n",
    "print(\"Best Parameters from gridsearch: {%s} with a score of %0.4f\" % (CV_rfc.best_params_, CV_rfc.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score is 0.469953775039\n",
      "Accuracy is 0.842857142857\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "model = CV_rfc.best_estimator_\n",
    "y_pred = model.predict_proba(X_test) #probability that team0 wins (what Kaggle calls team 1, and wants for submission)\n",
    "y_pred_acc = model.predict(X_test)   #firm class vote by all the trees, 0 or 1 in this case.\n",
    "test_score = metrics.roc_auc_score(y_test, y_pred[:,1], average=\"weighted\") #area under curve from prediction scores\n",
    "test_score_acc = metrics.accuracy_score(y_test, y_pred_acc)\n",
    "print(\"AUC score is {0}\".format(test_score))\n",
    "print(\"Accuracy is {0}\".format(test_score_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score is not(!) a good indicator for this kind of problem since most of the entries will always be 0 (i.e. only 1 in 10 teams will win the world series)! Our y_pred_acc array is filled completely with 0's (i.e. not a single positive world series prediction) and we still get an 86% score. The random forest model sucks right now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Feature\\t\\tImportance\\n\")\n",
    "for i in reversed(np.argsort(model.feature_importances_)):\n",
    "    print(\"%s\\t\\t%f\" % (X.columns[i], model.feature_importances_[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Some analysis tools \n",
    "#n, bins, patches = plt.hist(pitching[\"ipouts\"].loc[pitching[\"hot_pitcher\"]==1]/(3*pitching[\"g\"].loc[pitching[\"hot_pitcher\"]==1]), 50)\n",
    "\n",
    "#pitching.loc[pitching[\"g\"]==0, \"g\"] = 1\n",
    "\n",
    "pitching.loc[pd.isnull(pitching[\"ip/g\"])]\n",
    "\n",
    "#pitching[\"ip/g\"] = pitching[\"ipouts\"]/(3*pitching[\"g\"])\n",
    "n, bins, patches = plt.hist(pitching[\"ip/g\"], 50)\n",
    "#plt.xlim([0,10])\n",
    "\n",
    "#n, bins, patches = plt.hist(pitching[\"so_bb\"], 50)\n",
    "#plt.xlim([0,10])\n",
    "#pitching[\"so_bb\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
